{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6152ea69-351c-423b-a26f-25812745d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset's link.\n",
    "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13f7fecb-053c-4779-a397-88536f88b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def get_data(url):\n",
    "    \"\"\"get the dataset from url and save it locally.\"\"\"\n",
    "    #check if files exist.\n",
    "    if not os.path.exists('squad/train-v2.0.json') and not os.path.exists('squad/dev-v2.0.json'):\n",
    "        if not os.path.exists('squad'):\n",
    "            os.mkdir('squad')\n",
    "\n",
    "        for file in ['train-v2.0.json', 'dev-v2.0.json']:\n",
    "            res = requests.get(f'{url}{file}')\n",
    "            with open(f'./squad/{file}', 'wb') as f:\n",
    "                for chunk in res.iter_content(chunk_size=4):\n",
    "                    f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fdc31ec-5fd3-4411-b883-e46f5eebf0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_data(path):\n",
    "    \"\"\"get the dataset then return the data in a list of dict.\"\"\"\n",
    "    get_data(url)\n",
    "    \n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    #dict to store contexts, questions, and answers.\n",
    "    datas = {'question': [], 'id': [], 'answers': [], 'is_impossible': [], 'context': []}\n",
    "\n",
    "    #iterate through all data in squad.\n",
    "    for data in squad_dict['data']:\n",
    "        for paragraph in data['paragraphs']:\n",
    "            context = paragraph['context'].strip()\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question'].strip()\n",
    "                \n",
    "                datas['question'].append(question)\n",
    "                datas['id'].append(qa['id'])\n",
    "                datas['answers'].append(qa['answers'])\n",
    "                datas['is_impossible'].append(qa['is_impossible'])\n",
    "                datas['context'].append(context)\n",
    "                \n",
    "    #return formatted lists of data.\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0aed93f-e8a2-4f1d-be20-2a38b012a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data('./squad/train-v2.0.json')\n",
    "test_data = read_data('./squad/dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d13743-9b79-4f17-bb9e-bcd99f970e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "#name of bert model to use.\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9f37663-40ac-4ee9-b6db-9769401fd830",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 #the maximum length of a feature (question and context) - 3/4 of the model's maximum length.\n",
    "doc_stride = 128 #the overlap between two part of the context when splittingis needed - since answer may lie at the splitting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8683a3f7-831e-41da-9631-f6dfe579cac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, max_length, doc_stride):\n",
    "    encodings = tokenizer(data['question'],\n",
    "                          data['context'],\n",
    "                          max_length=max_length,\n",
    "                          truncation='only_second',\n",
    "                          stride=doc_stride,\n",
    "                          return_overflowing_tokens=True,\n",
    "                          return_offsets_mapping=True,\n",
    "                          padding='max_length')\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76dc9bdf-8a06-4985-935a-64736b8eb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = encode(train_data, max_length, doc_stride)\n",
    "test_encodings = encode(test_data, max_length, doc_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5794c888-9445-4bb5-a137-9828e3fa3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_mapping = train_encodings.pop('overflow_to_sample_mapping')\n",
    "train_offset_mapping = train_encodings.pop('offset_mapping')\n",
    "\n",
    "test_sample_mapping = test_encodings.pop('overflow_to_sample_mapping')\n",
    "test_offset_mapping = test_encodings.pop('offset_mapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f1c48f2-79eb-495b-b41e-63ae0ca68d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_position_tokens(data, encodings, sample_mapping, offset_mapping):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, mapping_idx in enumerate(sample_mapping):\n",
    "        start_pos = []\n",
    "        end_pos = []\n",
    "        answer = data['answers'][mapping_idx]\n",
    "        offset = offset_mapping[i]\n",
    "        if len(answer): #has an answer.\n",
    "            answer = answer[0] #training data has at most 1 answer for each question.\n",
    "            start_char = answer['answer_start']\n",
    "            end_char = start_char + len(answer['text'])\n",
    "            sequence_ids = encodings.sequence_ids(i)\n",
    "\n",
    "            #find the start and end of the answer in context.\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            #if the answer is not fully inside the context, label it (0,0).\n",
    "            if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                #otherwise it's the start and end token positions.\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "        else: #no answers, label with (0,0).\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            \n",
    "    encodings['start_positions'] = start_positions\n",
    "    encodings['end_positions'] = end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eef85bf0-43c1-4337-a57d-846dc8d8f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_position_tokens(train_data, train_encodings, train_sample_mapping, train_offset_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b391988c-07c8-4b4e-ad0b-42acd82790f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da8106c3-3ce5-42c5-8be8-544ced4c85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(train_encodings)\n",
    "test_dataset = SquadDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c223661e-aa7e-4d7c-b21c-5ccd14281de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def process_predictions(raw_predictions, test_data, test_encodings, test_sample_mapping, test_offset_mapping, n_best_size=20, max_answer_length=30):\n",
    "    predictions = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(raw_predictions):\n",
    "        valid_answers = []\n",
    "        min_null_score = None #if is_impossible, the correct answer is set to (0,0), this will compare the score at (0,0) with the best score produced.\n",
    "        start_logits = []\n",
    "        end_logits = []\n",
    "        offset_mappings = []\n",
    "        sequence_ids = []\n",
    "\n",
    "        idx = 0 #store the number of features map to this test_data['answers'].\n",
    "        #store all the logits belong to this mapping index in test_data['answers'].\n",
    "        while test_sample_mapping[i+idx]==test_sample_mapping[i]:\n",
    "            start_logits.append(raw_predictions[i+idx][0].cpu().numpy()) #get all start logits.\n",
    "            end_logits.append(raw_predictions[i+idx][1].cpu().numpy()) #get all end logits.\n",
    "            offset_mappings.append(test_offset_mapping[i+idx]) #get all offsets.\n",
    "            sequence_ids.append(test_encodings.sequence_ids(i+idx))\n",
    "            idx += 1\n",
    "            if i + idx >= len(raw_predictions): #the very last iteration.\n",
    "                break\n",
    "        \n",
    "        #go through the features map to this test_data['answers'].\n",
    "        for j in range(idx):\n",
    "            #update minimum null prediction.\n",
    "            cls_index = test_encodings['input_ids'][test_sample_mapping[i+j]].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[j][cls_index] + end_logits[j][cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            #a list of possible start/end indexes.\n",
    "            start_indexes = np.argsort(start_logits[j])[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits[j])[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    #out of length or not in context indexes.\n",
    "                    if (start_index >= len(offset_mappings[j]) or \n",
    "                        end_index >= len(offset_mappings[j]) or \n",
    "                        sequence_ids[j][start_index] != 1 or \n",
    "                        sequence_ids[j][end_index] != 1):\n",
    "                        continue\n",
    "                    #negative length or length greater than the set max length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mappings[j][start_index][0]\n",
    "                    end_char = offset_mappings[j][end_index][1]\n",
    "                    valid_answers.append({'score': start_logits[j][start_index] + end_logits[j][end_index],\n",
    "                                          'text': test_data['context'][test_sample_mapping[i+j]][start_char:end_char]})\n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x['score'], reverse=True)[0]\n",
    "        else:\n",
    "            best_answer = {'text': \"\", 'score': 0.0} #dummy for rare edge case.\n",
    "\n",
    "        predictions.append(best_answer['text'] if best_answer['score'] > min_null_score else \"\")\n",
    "        \n",
    "        #skip idx iterations since it has already been dealt with.\n",
    "        i += idx\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64c1b02a-508f-4a37-919a-1931457d34bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "#the below functions are from evaluation script at: https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def get_raw_scores(data, preds):\n",
    "    exact_scores = []\n",
    "    f1_scores = []\n",
    "    for i, answers in enumerate(test_data['answers']):\n",
    "        gold_answers = [a['text'] for a in answers if normalize_answer(a['text'])]\n",
    "        if not gold_answers:\n",
    "            gold_answers = ['']\n",
    "        exact_scores.append(max(compute_exact(a, preds[i]) for a in gold_answers))\n",
    "        f1_scores.append(max(compute_f1(a, preds[i]) for a in gold_answers))\n",
    "    return exact_scores, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea164950-484e-47fb-a999-bef2038ec96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader):\n",
    "    #evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    #store variables.\n",
    "    raw_predictions = []\n",
    "\n",
    "    for batch in tqdm(test_dataloader):   \n",
    "        #load into device.\n",
    "        batch = tuple(batch[b].to(device) for b in batch if b != 'token_type_ids')\n",
    "\n",
    "        #define inputs.\n",
    "        inputs = {'input_ids':       batch[0],\n",
    "                  'attention_mask':  batch[1]}\n",
    "\n",
    "        #compute logits.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        #get predictions.\n",
    "        start_pred = outputs.start_logits\n",
    "        end_pred = outputs.end_logits\n",
    "        \n",
    "        for i in range(len(start_pred)):\n",
    "            prediction = (start_pred[i], end_pred[i])\n",
    "            #for computing accuracy.  \n",
    "            raw_predictions.append(prediction)\n",
    "\n",
    "    return raw_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cb3f846-7ba5-4fef-b79b-662b72ad29cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 2070 Super\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 1.3042786912306121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1517/1517 [03:12<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: 0.6717762991661753\n",
      "F1 score: 0.705992944644306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.7602272240335317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1517/1517 [03:14<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: 0.7064768803166849\n",
      "F1 score: 0.7374936603764435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.47304157943793634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1517/1517 [03:11<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: 0.6976332856059968\n",
      "F1 score: 0.7326860566593234\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 8\n",
    "#use data loader.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#set up optimizer and scheduler for fine-tuning model.\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=3e-5,\n",
    "                  eps=1e-8,\n",
    "                  weight_decay=1e-2)\n",
    "epochs = 3\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(train_dataloader)*epochs)\n",
    "\n",
    "#setup GPU/CPU.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "#move model to device.\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    #set model to training mode.\n",
    "    model.train()\n",
    "    \n",
    "    #setting initial loss to 0.\n",
    "    loss_train_total = 0\n",
    "    \n",
    "    #setting up the progress bar.\n",
    "    progress_bar = tqdm(train_dataloader,\n",
    "                        desc='Epoch {:1d}'.format(epoch),\n",
    "                        leave=False,\n",
    "                        disable=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        #set gradient to 0.\n",
    "        model.zero_grad()\n",
    "\n",
    "        #the batch will be a tuple of 4.\n",
    "        batch = tuple(batch[b].to(device) for b in batch if b != 'token_type_ids')\n",
    "\n",
    "        #dictionary of inputs.\n",
    "        inputs = {'input_ids':       batch[0],\n",
    "                  'attention_mask':  batch[1],\n",
    "                  'start_positions': batch[2],\n",
    "                  'end_positions':   batch[3]}\n",
    "        \n",
    "        #unpack the dict straight into inputs.\n",
    "        outputs = model(**inputs)\n",
    "        #extract loss.\n",
    "        loss = outputs.loss\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        #gradient clipping.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "        \n",
    "    #check if path exists.\n",
    "    if not os.path.exists('models'):\n",
    "        os.mkdir('models')\n",
    "    torch.save(model.state_dict(), f'models/Bert_qa_ft_epoch{epoch}.model')\n",
    "\n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "\n",
    "    loss_train_ave = loss_train_total/len(train_dataloader)\n",
    "    tqdm.write(f'Training loss: {loss_train_ave}')\n",
    "    \n",
    "    raw_predictions = evaluate(model, test_dataloader)\n",
    "    predictions = process_predictions(raw_predictions, \n",
    "                                      test_data,\n",
    "                                      test_encodings, \n",
    "                                      test_sample_mapping, \n",
    "                                      test_offset_mapping, \n",
    "                                      n_best_size=20, \n",
    "                                      max_answer_length=30)\n",
    "    exact_scores, f1_scores = get_raw_scores(test_data, predictions)\n",
    "    exact_score, f1_score = np.average(exact_scores), np.average(f1_scores)\n",
    "    tqdm.write(f'Exact match: {exact_score}')\n",
    "    tqdm.write(f'F1 score: {f1_score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
